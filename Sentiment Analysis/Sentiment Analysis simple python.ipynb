{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO0lT/4wLQPIjbZTZzGkQ63"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kcms-dggCPJ8","executionInfo":{"status":"ok","timestamp":1711981398662,"user_tz":-60,"elapsed":13132,"user":{"displayName":"Siddharth Asthana","userId":"02587652800522319733"}},"outputId":"1a68d00d-6be1-4f24-d10e-4c729c7be1f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"]}],"source":["pip install scikit-learn nltk"]},{"cell_type":"code","source":["import nltk                                             #Natural language tooolkit for NLP tasks/Preprocessing text data\n","nltk.download('punkt')\n","import string                                           # import string module for string operations\n","import re                                               # Import regular expressions module\n","\n","# Import stopwords corpus from nltk to remove stopwords (the,is,are etc) during preprocessing\n","from nltk.corpus import stopwords\n","\n","# Import TfidfVectorizer class from the sklearn.feature_extraction.text module\n","# TfidfVectorizer is a utility class in SciKit-learn(sklearn) module for converting text into a matrix of TF-IDF features\n","# TfidfVectorizer converts each doc into feature vector cz ML understand no. not text.\n","# TfidfVectorizer is commonly used as a preprocessing step in machine learning pipelines\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# for splitting the dataset into train and test sets to evaluate performance\n","from sklearn.model_selection import train_test_split\n","\n","# Pipeline chains multiple processing steps (like data preprocessing, feature extraction, and model training)together into a single object\n","from sklearn.pipeline import Pipeline\n","\n","# Import Special vector classification class from sklearn.svm\n","# Supervised learning algorithm used for classification tasks.\n","from sklearn.svm import SVC\n","\n","# import the RandomForestClassifier class from the sklearn.ensemble module\n","# Random Forest is an ensemble learning method that constructs multiple decision trees during training\n","# Its output is the mode of the classes (classification) or the mean prediction (regression) of individual trees.\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# To calculate the accuarcy and print a report\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# import movie_reviews corpus from nltk\n","from nltk.corpus import movie_reviews\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aU5o3HItCrMS","executionInfo":{"status":"ok","timestamp":1711997826450,"user_tz":-60,"elapsed":281,"user":{"displayName":"Siddharth Asthana","userId":"02587652800522319733"}},"outputId":"a24b67c3-6b2f-4853-f335-5c4837bc2784"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["#Loading the dataset movie_reviews\n","nltk.download ('movie_reviews')\n","\n","# Nested loop. Outer loop iterating over (file id, and category) using the movie_reviews.categories() function.\n","# This function returns a list of categories, which in this case are 'pos' (positive) and 'neg' (negative).\n","\n","#Inner loop that iterates over each file id associated with the current category movie_reviews.fileids(category) function\n","# returns a list of file IDs for a given category.\n","documents = [(movie_reviews.raw(fileid), category)\n","              for category in movie_reviews.categories()\n","              for fileid in movie_reviews.fileids(category)]\n","\n","# Seperate the reviews and labels\n","\n","# Creates a list [array] X containing only the raw text of the movie reviews from the documents list\n","X = [review for review, _ in documents]\n","\n","# Creates a list Y containing only the categories (labels) of the movie reviews from the documents list\n","Y = [category for _, category in documents]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w4pEqgNTED7Y","executionInfo":{"status":"ok","timestamp":1711997831360,"user_tz":-60,"elapsed":786,"user":{"displayName":"Siddharth Asthana","userId":"02587652800522319733"}},"outputId":"4c9e4419-5a82-40b8-8016-b58f00ddaa3b"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Package movie_reviews is already up-to-date!\n"]}]},{"cell_type":"code","source":["nltk.download('wordnet')       #Used for lemmatization\n","#Downloads the stopwords corpus to be used in preprocessing\n","nltk.download('stopwords')\n","\n","# Text preprocessing function\n","def preprocess_text(text):\n","    # Tokenization: Split the text into words or tokens\n","    # This is an assignemtn statement. Tokens is a variable.\n","    # This function call will invoke tokenization method word_tokenize() in the nltk\n","    tokens = word_tokenize(text)\n","\n","    # Lowercasing: Convert all tokens to lowercase\n","    # This is an assignemtn statement. Tokens is again a variable\n","    # This is a list [] comprehension statement, which iterates over each element of token in tokens list\n","    # It calls the lower() method on each token\n","    tokens = [token.lower() for token in tokens]\n","\n","    # Remove punctuation: Remove punctuation tokens\n","    # Assignment statement which iterates over each token in tokens list and includes it in the new tokens lists if they are not punctuation marks\n","    tokens = [token for token in tokens if token not in string.punctuation]\n","\n","    # Remove stopwords: Remove common stopwords\n","\n","    # Calls the words() function in the stopwords module in nltk.\n","    # this function returns a list of english keywords\n","    stopwords_list = set(stopwords.words('english'))\n","\n","    # Assignment st. iterating over each token in tokens list and includes the tokens in new list only if they are not stopwords\n","    tokens = [token for token in tokens if token not in stopwords_list]\n","\n","    # Lemmatization: Convert tokens to their base form\n","    # wordnetlemmatizer is a class. We are creating an object or instance 'lemmatizer' for the class\n","    lemmatizer = WordNetLemmatizer()\n","    # Assignment st that iterates on each token in tokens list.\n","    # it calls lemmatizer() function for each token\n","    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","\n","    # Join tokens back into text\n","    # Join() is a function. We are joining each token into a single string seperated by a single space ' '\n","    preprocessed_text = ' '.join(tokens)\n","\n","    # returns the single string just created\n","    return preprocessed_text\n","\n","# Preprocess the text data\n","# this is list comprehension, a concise way to apply an operation to each element in the list.\n","# it is iterating over each text in list X asking to implement each preprocessing step defined above\n","X = [preprocess_text(text) for text in X]\n","\n","# Split the data into train and test set\n","#Train_test_split() is a function. We have passed X, Y lists into it. Test dateset size is 20%.\n","# We are then assigning the 4 split datasets to 4 variables.\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n","\n","# Define the pipeline\n","# initializes Pipeline() object, which applies one step sequencially and feeds the output as an input to the next step\n","# tfidf is the first step. It consists of TfidfVectorizer() function which converts text data into matrix of tf-idf features.\n","# Text Feature extraction.\n","# clf is the next step. it calls special vector classifier SVC() fucntion with linear kernel\n","# The classifier component (clf) of the pipeline is responsible for learning patterns from the input features (TF-IDF vectors)\n","# and making predictions or decisions based on those patterns.\n","pipeline = Pipeline([\n","    ('tfidf', TfidfVectorizer(ngram_range=(1, 2), max_features=5000)),\n","    ('clf', SVC(kernel='linear'))\n","])\n","\n","# Train the model\n","# Calling fit() method in the pipeline object. it applies each step in the pipeline to the passed variables\n","pipeline.fit(X_train, Y_train)\n","\n","# Make predictions\n","# Calling predict() method in pipeline object. It applies trained pipeline to X_test and generate patterns based on learned patterns.\n","Y_pred = pipeline.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(Y_test, Y_pred)\n","print (\"Accuracy=\", accuracy)\n","\n","# Display classification report\n","print (\"Classification report:\")\n","print (classification_report(Y_test, Y_pred))\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GsmI2QXEHJlE","executionInfo":{"status":"ok","timestamp":1711997879124,"user_tz":-60,"elapsed":38260,"user":{"displayName":"Siddharth Asthana","userId":"02587652800522319733"}},"outputId":"d823121b-3547-4a45-f710-3de21b353acb"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy= 0.8375\n","Classification report:\n","              precision    recall  f1-score   support\n","\n","         neg       0.85      0.82      0.83       199\n","         pos       0.83      0.86      0.84       201\n","\n","    accuracy                           0.84       400\n","   macro avg       0.84      0.84      0.84       400\n","weighted avg       0.84      0.84      0.84       400\n","\n"]}]},{"cell_type":"code","source":["from collections import Counter\n","\n","# Count the number of positive and negative reviews\n","review_counts = Counter(Y)\n","print(\"Review Distribution:\", review_counts)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0tpZFshsL-u3","executionInfo":{"status":"ok","timestamp":1711985025456,"user_tz":-60,"elapsed":286,"user":{"displayName":"Siddharth Asthana","userId":"02587652800522319733"}},"outputId":"0c80b363-3865-46b4-c9c7-ee7ec99944b5"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Review Distribution: Counter({'neg': 1000, 'pos': 1000})\n"]}]},{"cell_type":"code","source":["import nltk\n","import string\n","import re\n","from nltk.corpus import stopwords, movie_reviews\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Download the movie_reviews dataset\n","nltk.download('movie_reviews')\n","\n","# Load the movie reviews\n","documents = [(movie_reviews.raw(fileid), category)\n","             for category in movie_reviews.categories()\n","             for fileid in movie_reviews.fileids(category)]\n","\n","# Separate the reviews and labels\n","X = [review for review, _ in documents]\n","y = [category for _, category in documents]\n","\n","# Text preprocessing function\n","def preprocess_text(text):\n","    # Convert to lowercase\n","    text = text.lower()\n","    # Remove punctuation and numbers\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    # Remove stopwords\n","    stopwords_list = set(stopwords.words('english'))\n","    text = ' '.join(word for word in text.split() if word not in stopwords_list)\n","    # Remove any additional whitespace\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","# Preprocess the text data\n","X = [preprocess_text(text) for text in X]\n","\n","# Split the data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define the pipeline\n","pipeline = Pipeline([\n","    ('tfidf', TfidfVectorizer(ngram_range=(1, 2), max_features=5000)),\n","    ('clf', SVC())\n","])\n","\n","# Define the hyperparameters grid for grid search\n","param_grid = {\n","    'clf__C': [0.1, 1, 10],\n","    'clf__kernel': ['linear', 'rbf']\n","}\n","\n","# Perform grid search with cross-validation\n","grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)\n","grid_search.fit(X_train, y_train)\n","\n","# Best parameters found by grid search\n","print(\"Best Parameters:\", grid_search.best_params_)\n","\n","# Make predictions on the test set using the best model\n","best_model = grid_search.best_estimator_\n","y_pred = best_model.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)\n","\n","# Display classification report\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5MPw6fpdN5bX","executionInfo":{"status":"ok","timestamp":1711998105454,"user_tz":-60,"elapsed":200417,"user":{"displayName":"Siddharth Asthana","userId":"02587652800522319733"}},"outputId":"a436c00f-ea9b-4863-f571-ae93da9151c1"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Package movie_reviews is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Best Parameters: {'clf__C': 1, 'clf__kernel': 'linear'}\n","Accuracy: 0.84\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","         neg       0.85      0.83      0.84       199\n","         pos       0.83      0.85      0.84       201\n","\n","    accuracy                           0.84       400\n","   macro avg       0.84      0.84      0.84       400\n","weighted avg       0.84      0.84      0.84       400\n","\n"]}]}]}